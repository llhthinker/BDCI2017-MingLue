{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import collections\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    max_sentence_num = 500\n",
    "    max_sentence_len= 100\n",
    "    vocab_size = 100000\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_path, config):\n",
    "    \"\"\"\n",
    "    载入数据\n",
    "    \"\"\"\n",
    "    data= []\n",
    "    labels = []\n",
    "    max_sentence_num = 0\n",
    "    max_sentence_len = 0\n",
    "    longest_sentence = \"\"\n",
    "    with open(data_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line_list = line.split('\\t')\n",
    "            sentences = re.split(r'[。|；|！|？|，|（|）]', line_list[1])\n",
    "            tmp_num = len(sentences)\n",
    "            if tmp_num > max_sentence_num:\n",
    "                max_sentence_num = tmp_num\n",
    "            if tmp_num > config.max_sentence_num:\n",
    "                continue\n",
    "            one_data = []\n",
    "            for s in sentences:\n",
    "                words = s.split(' ')\n",
    "                tmp_len = len(words)\n",
    "                if tmp_len > max_sentence_len:\n",
    "                    max_sentence_len = tmp_len\n",
    "                    longest_sentence = words\n",
    "                one_data.append(words)\n",
    "            data.append(one_data[:config.max_sentence_len])\n",
    "            labels.append(int(line_list[2]))\n",
    "        f.close()\n",
    "    print(\"max sentence num:\", max_sentence_num)\n",
    "    print(\"max sentence length: \", max_sentence_len)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence num: 2250\n",
      "max sentence length:  257\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/seg_sample_train.txt'\n",
    "data, labels = load_data(data_path, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['公诉', '机关', '北京市', '昌平区', '人民检察院', ''], ['', '被告人', '呼', '&', 'times', ';', '&', 'times', ';', ''], ['', '男', ''], ['', '27', '岁', ''], ['', '1986', '年', '11', '月', '10', '日', '出生', ''], ['', ''], ['', '因涉嫌', '犯', '盗窃罪', '于', '2014', '年', '6', '月', '20', '日', '被', '羁押', ''], ['', '同年', '7', '月', '3', '日', '被', '逮捕', ''], ['', '现', '羁押于', '北京市', '昌平区', '看守所', ''], ['', '北京市', '昌平区', '人民检察院', '以京昌检', '公诉', '刑诉', ''], ['', '2014', ''], ['', '610', '号', '起诉书', '指控', '被告人', '呼', '&', 'times', ';', '&', 'times', ';', '犯', '盗窃罪', ''], ['', '于', '2014', '年', '7', '月', '22', '日向', '本院', '提起公诉', ''], ['', '本院', '依法', '适用', '简易程序', ''], ['', '实行', '独任', '审判', ''], ['', '公开', '开庭', '进行', '了', '审理', ''], ['', '北京市', '昌平区', '人民检察院', '指派', '检察员', '夏', '文广', '出庭', '支持', '公诉', ''], ['', '被告人', '呼', '&', 'times', ';', '&', 'times', ';', '到庭', '参加', '诉讼', ''], ['', '本案', '现已', '审理', '终结', ''], ['', '北京市', '昌平区', '人民检察院', '起诉书', '指控', '：', '被告人', '呼', '&', 'times', ';', '&', 'times', ';', '于', '2014', '年', '6', '月', '20', '日', '16', '时', '50', '分许', ''], ['', '进入', '昌平区', '东小', '口镇', '兴旺', '地', '市场', '南门', '西侧', '赵', '&', 'times', ';', '&', 'times', ';', '烟', '酒店', '内', ''], ['', '趁', '四周', '无人', ''], ['', '将店', '内', '货架', '抽屉', '内', '的', '人民币', '5700', '余元', '及', '5000', '余元', '的', '手机', '充值卡', '窃走', ''], ['', '赵', '&', 'times', ';', '&', 'times', ';', '发现', '后', '随即', '追赶', ''], ['', '被告人', '呼', '&', 'times', ';', '&', 'times', ';', '在', '逃跑', '过程', '中', '分', '两次', '将', '所', '窃', '钱', '、', '卡', '抛撒', ''], ['', '后', '被', '抓获', ''], ['', '赵', '&', 'times', ';', '&', 'times', ';', '在', '追赶', '过程', '中', '捡拾', '被盗', '人民币', '1587', '元', ''], ['', '手机', '充值卡', '2960', '元', ''], ['', '上述事实', ''], ['', '被告人', '呼', '&', 'times', ';', '&', 'times', ';', '在', '庭审', '过程', '中未', '提出异议', ''], ['', '并', '有', '经过', '庭审', '质证', '、', '认证', '的', '被害人', '赵', '&', 'times', ';', '&', 'times', ';', '的', '陈述', ''], ['', '证人', '李', '&', 'times', ';', '&', 'times', ';', '、', '郭', '&', 'times', ';', '&', 'times', ';', '的', '证言', ''], ['', '辨认', '笔录', ''], ['', '公安机关', '出具', '的', '接', '报案', '和', '到案', '经过', ''], ['', '照片', ''], ['', '工作', '说明', ''], ['', '身份证明', '材料', ''], ['', '光盘', ''], ['', '被告人', '呼', '&', 'times', ';', '&', 'times', ';', '的', '供述', '等', '证据', '在案', '佐证', ''], ['', '足以认定', ''], ['']]\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_voabulary(data, vocab_size):\n",
    "    \"\"\"\n",
    "    基于所有数据构建词表\n",
    "    \"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    words = []\n",
    "    for doc in data:\n",
    "        for sentence in doc:\n",
    "            words.extend(sentence)\n",
    "    count.extend(collections.Counter(words).most_common(vocab_size - 1))\n",
    "    dict_word2index = dict()\n",
    "    for word, _ in count:\n",
    "        dict_word2index[word] = len(dict_word2index)\n",
    "    dict_index2word = dict(zip(dict_word2index.values(), dict_word2index.keys()))\n",
    "    \n",
    "    return  count, dict_word2index, dict_index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, dict_word2index, dict_index2word = build_voabulary(data, config.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(data, labels, dict_word2index, config):\n",
    "    \"\"\"\n",
    "    基于词表构建数据集（数值化）\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    indices = np.arange(len(labels))\n",
    "    np.random.shuffle(indices)\n",
    "    new_labels = []\n",
    "    for i in indices:\n",
    "        new_labels.append(labels[i]-1) \n",
    "        new_line = []\n",
    "        for word in data[i]:\n",
    "            if word in dict_word2index:\n",
    "                index = dict_word2index[word]\n",
    "            else:\n",
    "                index = 0    # UNK\n",
    "            new_line.append(index)\n",
    "        \n",
    "        zero_num = max_sentence_len - len(new_line)\n",
    "        while zero_num > 0:\n",
    "            new_line.append(0)\n",
    "            zero_num -= 1\n",
    "        dataset.append(new_line[:max_sentence_len])\n",
    "#     return dataset, new_labels\n",
    "    return np.array(dataset, dtype=np.int64), np.array(new_labels, dtype=np.int64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
