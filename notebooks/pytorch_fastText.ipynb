{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max sentence length:  20420\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"\n",
    "    载入数据\n",
    "    \"\"\"\n",
    "    data= []\n",
    "    labels = []\n",
    "    max_sentence_len = 0\n",
    "    with open(data_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line_list = line.split('\\t')\n",
    "            one_data = line_list[1].split(' ')\n",
    "            tmp_len = len(one_data)\n",
    "            if tmp_len > max_sentence_len:\n",
    "                max_sentence_len = tmp_len\n",
    "            data.append(one_data)\n",
    "            labels.append(int(line_list[2]))\n",
    "        f.close()\n",
    "    print(\"max sentence length: \", max_sentence_len)\n",
    "    return data, labels\n",
    "\n",
    "data_path = '../data/seg_sample_train.txt'\n",
    "data, labels = load_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-500:6015\n",
      "501-1000:2406\n",
      "1001-1500:647\n",
      "1501-2000:303\n",
      "2001-2500:206\n",
      "2501-3000:125\n",
      "3001-3500:70\n",
      "3501-4000:55\n",
      "4001-4500:37\n",
      "4501-5000:33\n",
      "5001-5500:19\n",
      "5501-6000:17\n",
      "6001-6500:9\n",
      "6501-7000:7\n",
      "7001-7500:5\n",
      "7501-8000:6\n",
      "8001-8500:7\n",
      "8501-9000:4\n",
      "9001-9500:7\n",
      "9501-10000:5\n",
      "10501-11000:2\n",
      "11501-12000:1\n",
      "12001-12500:2\n",
      "12501-13000:3\n",
      "14501-15000:3\n",
      "15001-15500:1\n",
      "16001-16500:1\n",
      "17001-17500:1\n",
      "17501-18000:2\n",
      "20001-20500:1\n"
     ]
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def show_text_len_distribution(data):\n",
    "    len_list = [len(text) for text in data]\n",
    "#     print(len_list[1:100])\n",
    "    step = 500\n",
    "    for k, g in groupby(sorted(len_list), key=lambda x: (x-1)//step):\n",
    "    #    dic['{}-{}'.format(k*step+1, (k+1)*step)] = len(list(g))\n",
    "        print('{}-{}'.format(k*step+1, (k+1)*step)+\":\"+str(len(list(g))))\n",
    "show_text_len_distribution(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_voabulary(data, vocabulary_size=50000):\n",
    "    \"\"\"\n",
    "    基于所有数据构建词表\n",
    "    \"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    words = []\n",
    "    for line in data:\n",
    "        words.extend(line)\n",
    "    for line in data:\n",
    "        words.extend(line)\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dict_word2index = dict()\n",
    "    for word, _ in count:\n",
    "        dict_word2index[word] = len(dict_word2index)\n",
    "    dict_index2word = dict(zip(dict_word2index.values(), dict_word2index.keys()))\n",
    "    \n",
    "    return  count, dict_word2index, dict_index2word\n",
    "\n",
    "count, dict_word2index, dict_index2word = build_voabulary(data, vocabulary_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(data, labels, dict_word2index, max_sentence_len=1000, label_size=8):\n",
    "    \"\"\"\n",
    "    基于词表构建数据集（数值化）\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    indices = np.arange(len(labels))\n",
    "    np.random.shuffle(indices)\n",
    "    new_labels = []\n",
    "    for i in indices:\n",
    "        new_labels.append(labels[i]-1) \n",
    "        new_line = []\n",
    "        for word in data[i]:\n",
    "            if word in dict_word2index:\n",
    "                index = dict_word2index[word]\n",
    "            else:\n",
    "                index = 0    # UNK\n",
    "            new_line.append(index)\n",
    "        \n",
    "        zero_num = max_sentence_len - len(new_line)\n",
    "        while zero_num > 0:\n",
    "            new_line.append(0)\n",
    "            zero_num -= 1\n",
    "        dataset.append(new_line[:max_sentence_len])\n",
    "#     return dataset, new_labels\n",
    "    return np.array(dataset, dtype=np.int64), np.array(new_labels, dtype=np.int64)\n",
    "\n",
    "train_data, train_labels = build_dataset(data, labels, dict_word2index, max_sentence_len=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 2 6 6 0 4 2 6 2 3]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 1000)\n",
      "(7000,)\n"
     ]
    }
   ],
   "source": [
    "def split_data(data, radio=0.7):\n",
    "    \"\"\"\n",
    "    将训练集分给为训练集和检验集\n",
    "    \"\"\"\n",
    "    split_index = int(len(data) * 0.7)\n",
    "    new_data1 = data[ : split_index]\n",
    "    new_data2 = data[split_index : ]\n",
    "    return new_data1, new_data2\n",
    "\n",
    "train_X, valid_X = split_data(train_data)\n",
    "train_y, valid_y = split_data(train_labels)\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class MingLueData(data.Dataset):\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.len = X.shape[0]\n",
    "        self.x_data = X\n",
    "        self.y_data = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "batch_size = 4\n",
    "num_workers = 2\n",
    "dataset = MingLueData(train_X, train_y)\n",
    "train_loader = data.DataLoader(dataset=dataset, \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=False,\n",
    "                               num_workers=num_workers)\n",
    "dataset = MingLueData(valid_X, valid_y)\n",
    "valid_loader = data.DataLoader(dataset=dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText (\n",
      "  (embedding): Embedding(100000, 128)\n",
      "  (linear): Linear (128 -> 8)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, num_class):\n",
    "        super(FastText, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_class = num_class\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, \n",
    "                                      embedding_dim=self.embedding_size)\n",
    "        self.linear = nn.Linear(in_features=self.embedding_size, \n",
    "                                out_features=self.num_class)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embed = self.embedding(text)\n",
    "        text_embed = torch.mean(embed, dim=1)\n",
    "#         print(text_embed.size())\n",
    "        text_embed = text_embed.view(-1, text_embed.size(2))\n",
    "        logits = self.linear(text_embed)\n",
    "        return logits    \n",
    "\n",
    "vocab_size = 100000\n",
    "embedding_size = 128\n",
    "num_class = 8\n",
    "fast_text = FastText(vocab_size=vocab_size, embedding_size=embedding_size,\n",
    "                    num_class=num_class)\n",
    "print(fast_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.001\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=fast_text.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.964\n",
      "[1,   200] loss: 1.912\n",
      "[1,   300] loss: 1.881\n",
      "[1,   400] loss: 1.881\n",
      "[1,   500] loss: 1.882\n",
      "[1,   600] loss: 1.820\n",
      "[1,   700] loss: 1.886\n",
      "[1,   800] loss: 1.826\n",
      "[1,   900] loss: 1.810\n",
      "[1,  1000] loss: 1.814\n",
      "[1,  1100] loss: 1.852\n",
      "[1,  1200] loss: 1.825\n",
      "[1,  1300] loss: 1.789\n",
      "[1,  1400] loss: 1.845\n",
      "[1,  1500] loss: 1.809\n",
      "[1,  1600] loss: 1.817\n",
      "[1,  1700] loss: 1.775\n",
      "[2,   100] loss: 1.808\n",
      "[2,   200] loss: 1.788\n",
      "[2,   300] loss: 1.774\n",
      "[2,   400] loss: 1.804\n",
      "[2,   500] loss: 1.814\n",
      "[2,   600] loss: 1.734\n",
      "[2,   700] loss: 1.817\n",
      "[2,   800] loss: 1.737\n",
      "[2,   900] loss: 1.733\n",
      "[2,  1000] loss: 1.728\n",
      "[2,  1100] loss: 1.750\n",
      "[2,  1200] loss: 1.734\n",
      "[2,  1300] loss: 1.690\n",
      "[2,  1400] loss: 1.755\n",
      "[2,  1500] loss: 1.726\n",
      "[2,  1600] loss: 1.722\n",
      "[2,  1700] loss: 1.681\n",
      "[3,   100] loss: 1.719\n",
      "[3,   200] loss: 1.689\n",
      "[3,   300] loss: 1.670\n",
      "[3,   400] loss: 1.718\n",
      "[3,   500] loss: 1.719\n",
      "[3,   600] loss: 1.622\n",
      "[3,   700] loss: 1.720\n",
      "[3,   800] loss: 1.638\n",
      "[3,   900] loss: 1.643\n",
      "[3,  1000] loss: 1.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-6:\n",
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/llh/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/llh/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/llh/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/llh/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/llh/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/llh/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/llh/anaconda3/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/llh/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 35, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/llh/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/llh/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/llh/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/llh/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5130355893f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/llh/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_num = 3\n",
    "for epoch in range(epoch_num):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        texts, labels = data\n",
    "#         print(labels.size())\n",
    "        inputs, labels = Variable(texts), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = fast_text(inputs)\n",
    "        loss = loss_fun(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def micro_avg_f1(predict_label, true_label, num_class):\n",
    "    N = len(predict_label)\n",
    "    m = num_class\n",
    "    w = Counter(true_label)\n",
    "    print(w)\n",
    "    score = 0\n",
    "    for i in range(m):\n",
    "        score += w[i] * f1(predict_label, true_label, i)\n",
    "\n",
    "    return score / float(N)\n",
    "\n",
    "\n",
    "def f1(predict_label, true_label, cur_label):\n",
    "    true_pos, false_pos = 0, 0\n",
    "    false_neg = 0\n",
    "    for i in range(len(predict_label)):\n",
    "        if predict_label[i] == cur_label:\n",
    "            if true_label[i] == cur_label:\n",
    "                true_pos += 1\n",
    "            else:\n",
    "                false_pos += 1\n",
    "        else:  # predict_label != cur_label\n",
    "            if true_label[i] == cur_label:\n",
    "                false_neg += 1\n",
    "    if true_pos == 0:\n",
    "        precision, recall = 0, 0\n",
    "    else:\n",
    "        precision = true_pos / float(true_pos + false_pos)\n",
    "        recall = true_pos / float(true_pos + false_neg)\n",
    "    if precision == 0 or recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 5, 6, 4, 1, 1, 5, 6]\n",
      "[0, 0, 1, 5, 6, 2, 0, 2, 1, 6]\n",
      "Counter({6: 714, 1: 553, 0: 461, 5: 461, 4: 352, 2: 303, 3: 144, 7: 12})\n",
      "Micro-Averaged F1: 0.30705623764426043\n"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for data in valid_loader:\n",
    "    texts, labels = data\n",
    "    outputs = fast_text(Variable(texts))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    true_labels.extend(labels)\n",
    "    predicted = [i[0] for i in predicted]\n",
    "    predicted_labels.extend(predicted)\n",
    "\n",
    "print(true_labels[:10])\n",
    "print(predicted_labels[:10])\n",
    "print(\"Micro-Averaged F1:\",micro_avg_f1(predicted_labels, true_labels, num_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
